{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "PLN - P3.1 :: El amigo misterioso.ipynb",
   "provenance": [],
   "collapsed_sections": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VbccwTaZONtH",
    "outputId": "a89f70e1-0cc5-4333-d994-0af064ef3f27",
    "ExecuteTime": {
     "end_time": "2024-01-23T16:10:57.804364300Z",
     "start_time": "2024-01-23T16:10:57.768316500Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'google.colab'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 1\u001B[0m\n\u001B[1;32m----> 1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mgoogle\u001B[39;00m\u001B[38;5;21;01m.\u001B[39;00m\u001B[38;5;21;01mcolab\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m drive\n\u001B[0;32m      2\u001B[0m drive\u001B[38;5;241m.\u001B[39mmount(\u001B[38;5;124m'\u001B[39m\u001B[38;5;124m/content/drive/\u001B[39m\u001B[38;5;124m'\u001B[39m)\n\u001B[0;32m      3\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01msys\u001B[39;00m\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'google.colab'"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive/')\n",
    "import sys\n",
    "sys.path.append('/content/drive/MyDrive/Colab Notebooks/U3 :: PLN')"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# A review example on how 'CountVectorizer' works\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# list of text documents\n",
    "text = [ \"The quick brown fox jumped over the lazy dog.\" ]\n",
    "\n",
    "# create the vector object\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# tokenize and build the dictionary\n",
    "vectorizer.fit(text)\n",
    "print(vectorizer.vocabulary_)\n",
    "\n",
    "# encode document\n",
    "vector = vectorizer.transform(text)\n",
    "\n",
    "# summarize encoded vector\n",
    "print(vector.shape)\n",
    "print(type(vector))\n",
    "print(vector.toarray())\n",
    "\n",
    "# test text\n",
    "test = [\"The lazy dog is sleeping.\"]\n",
    "test_vectorized = vectorizer.transform(test)\n",
    "\n",
    "# convert a 2-dimension array into 1-dimensional array\n",
    "import numpy as np\n",
    "test_vectorized_flatten = list(np.ravel(test_vectorized.toarray()))\n",
    "\n",
    "# print the frequency of words in the test text\n",
    "print(\"\\nFrequency of words in the test text:\")\n",
    "for word, count in zip(vectorizer.get_feature_names_out(), test_vectorized_flatten):\n",
    "   print(\"Word: {0:15} Count: {1:1}\".format(word, count))\n",
    "\n"
   ],
   "metadata": {
    "id": "-3Q_dZCdz4ZA",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "6835b753-9041-47fe-dde3-3953333de025",
    "ExecuteTime": {
     "start_time": "2024-01-23T16:10:57.789198500Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "**1) Preparación del saco de palabras**"
   ],
   "metadata": {
    "id": "eCppR2FCOQCT"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# 3. Importamos el vectorizador y el clasificador\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    " \n",
    "# 2. Importamos los textos\n",
    "from goldman_emma_raw import *\n",
    "from henson_mathew_raw import *\n",
    "from wu_tingfang_raw import *\n",
    "\n",
    "friends_docs = goldman_docs + henson_docs + wu_docs\n",
    "\n",
    "# 4. Texto de la carta\n",
    "mystery_postcard = \"\"\"\n",
    "My friend,\n",
    "From the 10th of July to the 13th, a fierce storm raged, clouds of\n",
    "freeing spray broke over the ship, encasing her in a coat of icy mail,\n",
    "and the tempest forced all of the ice out of the lower end of the\n",
    "channel and beyond as far as the eye could see, but the _Roosevelt_\n",
    "still remained surrounded by ice. There is little use for money up here, \n",
    "and the place is seldom visited.\n",
    "Hope to see you soon. \n",
    "\"\"\"\n",
    "\n",
    "# 5. Definimos el vectorizador\n",
    "bow_vectorizer = CountVectorizer()\n",
    "\n",
    "# 6. Definimos el vector BoW para el corpus\n",
    "friends_vectors = bow_vectorizer.fit_transform(friends_docs)\n",
    "\n",
    "# 7. Imprimimos diferentes aspectos\n",
    "print(f'Dictionary: {bow_vectorizer.vocabulary_}')\n",
    "print(f'Only words: {bow_vectorizer.get_feature_names_out()}')\n",
    "print(f'\\nMatrix shape of the corpus: {friends_vectors.shape}')\n",
    "print(f'\\nN-dimension vector of the corpus:\\n {friends_vectors.toarray()}')\n",
    "print(f'\\n1-dimension vector for row 0 in the corpus:\\n {friends_vectors.toarray().tolist()[0]}')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5-3O4qoHUuWT",
    "outputId": "a36b4876-af46-4741-ac4a-4df4cbd101ca",
    "ExecuteTime": {
     "start_time": "2024-01-23T16:10:57.790674500Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# 8. Mostramos las palabras del diccionario para la primera frase en el corpus con alguna ocurrencia (es decir, se prescinde de los términos (columnas) con valor 0)\n",
    "\n",
    "print(\"\\nFirst paragraph:\\n\" + friends_docs[0] + \"\\n\")\n",
    "\n",
    "friends_matrix_0 = pd.DataFrame(data=friends_vectors[0].toarray(), columns=bow_vectorizer.get_feature_names_out())\n",
    "friends_matrix_0.loc[:, friends_matrix_0.any()]"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 266
    },
    "id": "lKXlsjRmVrwK",
    "outputId": "ba706605-638d-4fc7-9645-aa7cd7fff0ee",
    "ExecuteTime": {
     "end_time": "2024-01-23T16:10:57.854906200Z",
     "start_time": "2024-01-23T16:10:57.807644500Z"
    }
   },
   "execution_count": 3,
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'friends_docs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[3], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# 8. Mostramos las palabras del diccionario para la primera frase en el corpus con alguna ocurrencia (es decir, se prescinde de los términos (columnas) con valor 0)\u001B[39;00m\n\u001B[1;32m----> 3\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mFirst paragraph:\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m \u001B[38;5;241m+\u001B[39m \u001B[43mfriends_docs\u001B[49m[\u001B[38;5;241m0\u001B[39m] \u001B[38;5;241m+\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m      5\u001B[0m friends_matrix_0 \u001B[38;5;241m=\u001B[39m pd\u001B[38;5;241m.\u001B[39mDataFrame(data\u001B[38;5;241m=\u001B[39mfriends_vectors[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39mtoarray(), columns\u001B[38;5;241m=\u001B[39mbow_vectorizer\u001B[38;5;241m.\u001B[39mget_feature_names_out())\n\u001B[0;32m      6\u001B[0m friends_matrix_0\u001B[38;5;241m.\u001B[39mloc[:, friends_matrix_0\u001B[38;5;241m.\u001B[39many()]\n",
      "\u001B[1;31mNameError\u001B[0m: name 'friends_docs' is not defined"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "**2) Identificación del remitente de la carta**"
   ],
   "metadata": {
    "id": "80Fx0hCIOd4w"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# 9. Definimos el vector BoW para el texto de la carta\n",
    "mystery_vector = bow_vectorizer.transform(mystery_postcard.split(\".\"))\n",
    "\n",
    "# 10. Mostramos el BoW para el texto de la carta\n",
    "mpm = pd.DataFrame(data=mystery_vector.toarray(), columns=bow_vectorizer.get_feature_names_out())\n",
    "\n",
    "mystery_vector_matrix = mpm.loc[:, mpm.any()]\n",
    "mystery_vector_matrix"
   ],
   "metadata": {
    "id": "QlGAxPl1ZqBW",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 268
    },
    "outputId": "e26153ed-618c-4028-8d23-bd6b941e70ae",
    "ExecuteTime": {
     "start_time": "2024-01-23T16:10:57.820210900Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# 11. Creamos la variable \"friends_classifier\"\n",
    "friends_classifier = MultinomialNB()\n",
    "\n",
    "# 12. Etiquetamos los mensajes numéricamente para posibilitar el trabajo del clasificador\n",
    "friends_labels = [1] * len(goldman_docs) + [2] * len(henson_docs) + [3] * len(wu_docs)\n",
    "\n",
    "# 13. Entrenamos el clasificador\n",
    "friends_classifier.fit(friends_vectors, friends_labels)\n",
    " \n",
    "# 14-15. Hacemos la predicción\n",
    "predictions = friends_classifier.predict(mystery_vector)\n",
    "\n",
    "## Cuantas más líneas hayan asociadas a un amigo más probable es que el texto desconocido sea suyo\n",
    "from collections import Counter\n",
    "mystery_friend = Counter(predictions)\n",
    "\n",
    "print(\"\\nThe author of the postcard was from {}!\\n\".format(mystery_friend.most_common(1)[0][0]))"
   ],
   "metadata": {
    "id": "jEYLjY1PXYRA",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "21156a43-5821-4042-e29c-e5de137ee148",
    "ExecuteTime": {
     "start_time": "2024-01-23T16:10:57.821244400Z"
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# 16. Probabilidades de cada remitente para cada una de las frases\n",
    "# https://dev.to/rajat_naegi/simply-explained-predictproba-263i\n",
    "\n",
    "probs = friends_classifier.predict_proba(mystery_vector).tolist()\n",
    "\n",
    "friends = predictions.tolist()\n",
    "probs_max = [ max(prob) for prob in probs ]\n",
    "friends_probs = list(zip(friends, probs_max))\n",
    "print(f'Probabilities of each sentence to belong a to friend: {friends_probs}')"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wzWqpcFdJ7Cu",
    "outputId": "f7189634-99d2-47f5-9acf-7f56fca443b8",
    "ExecuteTime": {
     "start_time": "2024-01-23T16:10:57.822303500Z"
    }
   },
   "execution_count": null,
   "outputs": []
  }
 ]
}
