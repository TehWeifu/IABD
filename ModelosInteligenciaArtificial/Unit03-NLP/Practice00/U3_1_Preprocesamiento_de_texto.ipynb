{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wfMfNsPaFZ7Z"
   },
   "source": [
    "### **1. PREPROCESAMIENTO DE TEXTO** <br>\n",
    "**EJEMPLOS**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "s7n5JRzQFGww",
    "outputId": "23a0180c-b79b-4973-c1f0-aff818e667d9",
    "ExecuteTime": {
     "end_time": "2023-11-21T18:52:58.258877300Z",
     "start_time": "2023-11-21T18:52:58.223871300Z"
    }
   },
   "source": [
    "import re\n",
    "\n",
    "texto = \"    Esto es un texto de prueba </p>\"\n",
    "salida = re.sub(\"r \\s{4}\", \"\", texto)\n",
    "print(salida)"
   ],
   "execution_count": 1,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Esto es un texto de prueba </p>\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ao2dtsWREnoh",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "d39e7872-4d04-4549-fb99-8079ead2e04f",
    "ExecuteTime": {
     "end_time": "2023-11-21T18:52:58.258877300Z",
     "start_time": "2023-11-21T18:52:58.229854100Z"
    }
   },
   "source": [
    "import re\n",
    "\n",
    "texto = \"<p> Esto es un texto de prueba </p>\"\n",
    "salida = re.sub(\"r<.?p>\", \"\", texto)\n",
    "print(salida)"
   ],
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<p> Esto es un texto de prueba </p>\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Pim1SsWmJLx5",
    "outputId": "f338f529-9827-476f-aa6a-fe8164c788b0",
    "ExecuteTime": {
     "end_time": "2023-11-21T18:52:58.258877300Z",
     "start_time": "2023-11-21T18:52:58.232633200Z"
    }
   },
   "source": [
    "cadena = 'eStO eS unA cAdeNA dE cAracTeREs'\n",
    "\n",
    "print(cadena.upper())\n",
    "# 'ESTO ES UNA CADENA DE CARACTERES'\n",
    "\n",
    "print(cadena.lower()) \n",
    "# 'esto es una cadena de caracteres'"
   ],
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ESTO ES UNA CADENA DE CARACTERES\n",
      "esto es una cadena de caracteres\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Instalamos uno de los modelos SpaCy\n",
    "# !pip install es_core_news_sm\n",
    "\n",
    "import nltk\n",
    "# Además, instalamos la librería NLTK para apoyar ciertas tareas de los ejemplos que siguen\n",
    "nltk.download('punkt')"
   ],
   "metadata": {
    "id": "2Mu86-LwPCYi",
    "ExecuteTime": {
     "end_time": "2023-11-21T18:52:59.151326400Z",
     "start_time": "2023-11-21T18:52:58.237887900Z"
    }
   },
   "execution_count": 4,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jsanchez\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": "True"
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zTq0i1yqKJZz",
    "outputId": "d5260fb8-41f3-4c21-fb6e-aff51663899a",
    "ExecuteTime": {
     "end_time": "2023-11-21T18:53:02.030478500Z",
     "start_time": "2023-11-21T18:52:59.151326400Z"
    }
   },
   "source": [
    "# TOKENIZACIÓN POR PALABRA\n",
    "import spacy\n",
    "\n",
    "\n",
    "# Cargar el modelo de idioma en español\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "\n",
    "# Texto que quieres tokenizar\n",
    "texto = \"Spacy es una excelente herramienta para procesamiento de lenguaje natural en español.\"\n",
    "\n",
    "# Aplicar el modelo para obtener los tokens\n",
    "doc = nlp(texto)\n",
    "\n",
    "# Imprimir los tokens\n",
    "for token in doc:\n",
    "    print(token.text)"
   ],
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python311\\Lib\\site-packages\\spacy\\util.py:910: UserWarning: [W095] Model 'es_core_news_sm' (3.1.0) was trained with spaCy v3.1.0 and may not be 100% compatible with the current version (3.7.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spacy\n",
      "es\n",
      "una\n",
      "excelente\n",
      "herramienta\n",
      "para\n",
      "procesamiento\n",
      "de\n",
      "lenguaje\n",
      "natural\n",
      "en\n",
      "español\n",
      ".\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cM75VwmrLBGr",
    "outputId": "b12c2bf2-b20a-477f-c972-ad54be541b53",
    "ExecuteTime": {
     "end_time": "2023-11-21T18:53:02.049956700Z",
     "start_time": "2023-11-21T18:53:02.030478500Z"
    }
   },
   "source": [
    "# TOKENIZACIÓN POR FRASE\n",
    "import spacy\n",
    "\n",
    "# Texto que quieres tokenizar\n",
    "texto = \"Spacy es una excelente herramienta para procesamiento de lenguaje natural en español. Puede tokenizar a nivel de oración también.\"\n",
    "\n",
    "# Aplicar el modelo para obtener el documento\n",
    "doc = nlp(texto)\n",
    "\n",
    "# Imprimir las oraciones\n",
    "for oracion in doc.sents:\n",
    "    print(oracion.text)\n"
   ],
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spacy es una excelente herramienta para procesamiento de lenguaje natural en español.\n",
      "Puede tokenizar a nivel de oración también.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "nk61TTdgLNWQ",
    "outputId": "b98d0653-3aa9-4b66-a471-b29cc2025cd3",
    "ExecuteTime": {
     "end_time": "2023-11-21T18:53:02.049956700Z",
     "start_time": "2023-11-21T18:53:02.040837200Z"
    }
   },
   "source": [
    "# LIMPIEZA DE TEXTO\n",
    "import spacy\n",
    "\n",
    "# Texto que quieres procesar\n",
    "texto = \"Spacy es una excelente herramienta para procesamiento de lenguaje natural en español.\"\n",
    "\n",
    "# Aplicar el modelo para obtener el documento\n",
    "doc = nlp(texto)\n",
    "\n",
    "# Filtrar las stopwords y imprimir las palabras restantes\n",
    "palabras_filtradas = [token.text for token in doc if not token.is_stop]\n",
    "print(\"Palabras sin stopwords:\", palabras_filtradas)\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ],
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palabras sin stopwords: ['Spacy', 'excelente', 'herramienta', 'procesamiento', 'lenguaje', 'natural', 'español', '.']\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Tf1G6MYX-4TG",
    "outputId": "6443fc6e-d49f-44b7-881c-4c48f9505aa5",
    "ExecuteTime": {
     "end_time": "2023-11-21T18:53:02.321827600Z",
     "start_time": "2023-11-21T18:53:02.049956700Z"
    }
   },
   "source": [
    "# DERIVACIÓN\n",
    "import spacy\n",
    "import nltk\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Descargar los recursos necesarios para NLTK\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Cargar el modelo de idioma en español de Spacy\n",
    "nlp = spacy.load('es_core_news_sm')\n",
    "\n",
    "# Crear un objeto PorterStemmer de NLTK\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Texto que quieres procesar\n",
    "texto = \"Spacy es una excelente herramienta para procesamiento de lenguaje natural en español.\"\n",
    "\n",
    "# Aplicar el modelo de Spacy para obtener el documento\n",
    "doc = nlp(texto)\n",
    "\n",
    "# Obtener las palabras lematizadas con Spacy y realizar \"stemming\" con NLTK\n",
    "palabras_stemming = [stemmer.stem(token.lemma_) for token in doc]\n",
    "\n",
    "print(\"Palabras lematizadas y stemmizadas:\", palabras_stemming)\n"
   ],
   "execution_count": 8,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\jsanchez\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "C:\\Python311\\Lib\\site-packages\\spacy\\util.py:910: UserWarning: [W095] Model 'es_core_news_sm' (3.1.0) was trained with spaCy v3.1.0 and may not be 100% compatible with the current version (3.7.2). If you see errors or degraded performance, download a newer compatible model or retrain your custom model with the current spaCy version. For more details and available updates, run: python -m spacy validate\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palabras lematizadas y stemmizadas: ['spaci', 'ser', 'uno', 'excelent', 'herramienta', 'para', 'procesamiento', 'de', 'lenguaj', 'natur', 'en', 'español', '.']\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "czsUu_qGCbhl",
    "outputId": "2f2962f1-a46c-4109-97cf-9fe87587eca6",
    "ExecuteTime": {
     "end_time": "2023-11-21T18:53:02.340501200Z",
     "start_time": "2023-11-21T18:53:02.323346300Z"
    }
   },
   "source": [
    "# LEMATIZACIÓN\n",
    "import spacy\n",
    "\n",
    "# Texto que quieres procesar\n",
    "texto = \"Spacy es una excelente herramienta para procesamiento de lenguaje natural en español.\"\n",
    "\n",
    "# Aplicar el modelo para obtener el documento\n",
    "doc = nlp(texto)\n",
    "\n",
    "# Imprimir las palabras lematizadas\n",
    "for token in doc:\n",
    "    print(\"Palabra: \", \"{:15}\".format(token.text), \"Lema: \", \"{:15}\".format(token.lemma_))\n",
    "\n"
   ],
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palabra:  Spacy           Lema:  Spacy          \n",
      "Palabra:  es              Lema:  ser            \n",
      "Palabra:  una             Lema:  uno            \n",
      "Palabra:  excelente       Lema:  excelente      \n",
      "Palabra:  herramienta     Lema:  herramienta    \n",
      "Palabra:  para            Lema:  para           \n",
      "Palabra:  procesamiento   Lema:  procesamiento  \n",
      "Palabra:  de              Lema:  de             \n",
      "Palabra:  lenguaje        Lema:  lenguaje       \n",
      "Palabra:  natural         Lema:  natural        \n",
      "Palabra:  en              Lema:  en             \n",
      "Palabra:  español         Lema:  español        \n",
      "Palabra:  .               Lema:  .              \n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tClkr5lXLqvZ",
    "outputId": "1d74e426-f939-49ed-b8ae-c217d87fbbd8",
    "ExecuteTime": {
     "end_time": "2023-11-21T18:53:02.340501200Z",
     "start_time": "2023-11-21T18:53:02.331247100Z"
    }
   },
   "source": [
    "# ETIQUETADO DEL DISCURSO\n",
    "\n",
    "# Texto que quieres procesar\n",
    "texto = \"Spacy es una excelente herramienta para procesamiento de lenguaje natural en español.\"\n",
    "\n",
    "# Aplicar el modelo para obtener el documento\n",
    "my_doc = nlp(texto)\n",
    "\n",
    "# Imprimir el etiquetado morfológico de cada palabra\n",
    "for token in my_doc:\n",
    "    print(\"Palabra: \", \"{:15}\".format(token.text), \"Etiqueta POS: \", \"{:15}\".format(token.pos_))\n"
   ],
   "execution_count": 10,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palabra:  Spacy           Etiqueta POS:  PROPN          \n",
      "Palabra:  es              Etiqueta POS:  AUX            \n",
      "Palabra:  una             Etiqueta POS:  DET            \n",
      "Palabra:  excelente       Etiqueta POS:  ADJ            \n",
      "Palabra:  herramienta     Etiqueta POS:  NOUN           \n",
      "Palabra:  para            Etiqueta POS:  ADP            \n",
      "Palabra:  procesamiento   Etiqueta POS:  NOUN           \n",
      "Palabra:  de              Etiqueta POS:  ADP            \n",
      "Palabra:  lenguaje        Etiqueta POS:  NOUN           \n",
      "Palabra:  natural         Etiqueta POS:  ADJ            \n",
      "Palabra:  en              Etiqueta POS:  ADP            \n",
      "Palabra:  español         Etiqueta POS:  NOUN           \n",
      "Palabra:  .               Etiqueta POS:  PUNCT          \n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: stanza in c:\\python311\\lib\\site-packages (1.6.1)\n",
      "Requirement already satisfied: emoji in c:\\python311\\lib\\site-packages (from stanza) (2.8.0)\n",
      "Requirement already satisfied: numpy in c:\\python311\\lib\\site-packages (from stanza) (1.26.0)\n",
      "Requirement already satisfied: protobuf>=3.15.0 in c:\\python311\\lib\\site-packages (from stanza) (4.24.3)\n",
      "Requirement already satisfied: requests in c:\\python311\\lib\\site-packages (from stanza) (2.31.0)\n",
      "Requirement already satisfied: torch>=1.3.0 in c:\\python311\\lib\\site-packages (from stanza) (2.1.1)\n",
      "Requirement already satisfied: tqdm in c:\\python311\\lib\\site-packages (from stanza) (4.66.1)\n",
      "Requirement already satisfied: filelock in c:\\python311\\lib\\site-packages (from torch>=1.3.0->stanza) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\python311\\lib\\site-packages (from torch>=1.3.0->stanza) (4.8.0)\n",
      "Requirement already satisfied: sympy in c:\\python311\\lib\\site-packages (from torch>=1.3.0->stanza) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\python311\\lib\\site-packages (from torch>=1.3.0->stanza) (3.1)\n",
      "Requirement already satisfied: jinja2 in c:\\python311\\lib\\site-packages (from torch>=1.3.0->stanza) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\python311\\lib\\site-packages (from torch>=1.3.0->stanza) (2023.10.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\python311\\lib\\site-packages (from requests->stanza) (3.3.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\python311\\lib\\site-packages (from requests->stanza) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\python311\\lib\\site-packages (from requests->stanza) (2.0.6)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\python311\\lib\\site-packages (from requests->stanza) (2023.7.22)\n",
      "Requirement already satisfied: colorama in c:\\python311\\lib\\site-packages (from tqdm->stanza) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\python311\\lib\\site-packages (from jinja2->torch>=1.3.0->stanza) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\python311\\lib\\site-packages (from sympy->torch>=1.3.0->stanza) (1.3.0)\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json:   0%|   …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "6d3e9406620e4d9397c5f2e07c2551a2"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-21 19:53:05 INFO: Downloading default packages for language: es (Spanish) ...\n",
      "2023-11-21 19:53:05 INFO: File exists: C:\\Users\\jsanchez\\stanza_resources\\es\\default.zip\n",
      "2023-11-21 19:53:08 INFO: Finished downloading models and saved to C:\\Users\\jsanchez\\stanza_resources.\n",
      "2023-11-21 19:53:08 INFO: Checking for updates to resources.json in case models have been updated.  Note: this behavior can be turned off with download_method=None or download_method=DownloadMethod.REUSE_RESOURCES\n"
     ]
    },
    {
     "data": {
      "text/plain": "Downloading https://raw.githubusercontent.com/stanfordnlp/stanza-resources/main/resources_1.6.0.json:   0%|   …",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "b03edbfc0bca4f938802e35b1ca7d29d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-11-21 19:53:09 INFO: Loading these models for language: es (Spanish):\n",
      "===============================\n",
      "| Processor | Package         |\n",
      "-------------------------------\n",
      "| tokenize  | ancora          |\n",
      "| mwt       | ancora          |\n",
      "| pos       | ancora_charlm   |\n",
      "| lemma     | ancora_nocharlm |\n",
      "===============================\n",
      "\n",
      "2023-11-21 19:53:09 INFO: Using device: cpu\n",
      "2023-11-21 19:53:09 INFO: Loading: tokenize\n",
      "2023-11-21 19:53:09 INFO: Loading: mwt\n",
      "2023-11-21 19:53:09 INFO: Loading: pos\n",
      "2023-11-21 19:53:09 INFO: Loading: lemma\n",
      "2023-11-21 19:53:09 INFO: Done loading processors!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Palabra:  esta            Lema:  este           \n",
      "Palabra:  es              Lema:  ser            \n",
      "Palabra:  la              Lema:  el             \n",
      "Palabra:  primera         Lema:  primero        \n",
      "Palabra:  prueba          Lema:  prueba         \n",
      "Palabra:  que             Lema:  que            \n",
      "Palabra:  realizo         Lema:  realizar       \n",
      "Palabra:  eliminando      Lema:  eliminar       \n",
      "Palabra:  palabras        Lema:  palabra        \n",
      "Palabra:  irrelevantes    Lema:  irrelevante    \n",
      "Palabra:  de              Lema:  de             \n",
      "Palabra:  una             Lema:  uno            \n",
      "Palabra:  oración         Lema:  oración        \n"
     ]
    }
   ],
   "source": [
    "# LEMATIZACIÓN EN ESPAÑOL\n",
    "# Ejemplo con la librería STANZA\n",
    "!pip install stanza\n",
    "\n",
    "import stanza as stz\n",
    "\n",
    "stz.download(\"es\")\n",
    "nlp = stz.Pipeline(lang='es', processors='tokenize,mwt,pos,lemma')\n",
    "\n",
    "sentence = \"Esta es la primera prueba que realizo eliminando palabras irrelevantes de una oración\"\n",
    "doc = nlp(sentence)\n",
    "words_lemmatized = [[word.text, word.lemma] for sent in doc.sentences for word in sent.words]\n",
    "for words in words_lemmatized:\n",
    "    print(\"Palabra: \", \"{:15}\".format(words[0]).lower(), \"Lema: \", \"{:15}\".format(words[1]).lower())\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-11-21T18:53:09.749258900Z",
     "start_time": "2023-11-21T18:53:02.340501200Z"
    }
   }
  }
 ]
}
