{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Transferencia de conocimiento\n",
    "\n",
    "En esta segunda práctica de transferencia de conocimiento vamos a practicar con distintas arquitecturas. Para ello vamos a emplear la base de datos de clasificación de imágenes en pnumonía vs normal: https://drive.google.com/file/d/1wgODntXVXLp5bS0IyP9HVpGOrykNpS3h/view\n",
    "\n",
    "Para la práctica debes de realizar los siguientes ejercicios:\n"
   ],
   "metadata": {
    "id": "waCyJW3rEcmc"
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "import os\n",
    "\n",
    "IMG_SIZE = (256, 256, 3)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "RANDOM_STATE = 123"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-06T18:21:37.084013Z",
     "start_time": "2024-03-06T18:21:37.080665Z"
    }
   },
   "execution_count": 134
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Ejercicio 1\n",
    "\n",
    "Descarga la base de datos y prepara los datasets de entrenamiento y validación con un tamaño de imagen de 256x256 y un tamaño de lote de 32."
   ],
   "metadata": {
    "id": "SDVa8UIRFCAG"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def get_images():\n",
    "    path_dataset = Path('./chest_xray')\n",
    "    path_train = path_dataset / 'entrenamiento'\n",
    "    path_val = path_dataset / 'validacion'\n",
    "\n",
    "    train_dataset = image_dataset_from_directory(\n",
    "        path_train,\n",
    "        shuffle=True,\n",
    "        seed=RANDOM_STATE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        image_size=(IMG_SIZE[0], IMG_SIZE[1])\n",
    "    )\n",
    "\n",
    "    validation_dataset = image_dataset_from_directory(\n",
    "        path_val,\n",
    "        shuffle=True,\n",
    "        seed=RANDOM_STATE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        image_size=(IMG_SIZE[0], IMG_SIZE[1])\n",
    "    )\n",
    "\n",
    "    return train_dataset, validation_dataset\n",
    "\n",
    "\n",
    "train_dataset, validation_dataset = get_images()"
   ],
   "metadata": {
    "id": "WujAg2TuFBSh",
    "ExecuteTime": {
     "end_time": "2024-03-06T18:21:37.713755Z",
     "start_time": "2024-03-06T18:21:37.112291Z"
    }
   },
   "execution_count": 135,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 500 files belonging to 2 classes.\n",
      "Found 200 files belonging to 2 classes.\n"
     ]
    },
    {
     "data": {
      "text/plain": "array([0, 1, 1, 0, 1, 1, 1, 0, 0, 1, 1, 0, 1, 1, 1, 0, 1, 1, 0, 1, 0, 0,\n       1, 1, 0, 1, 0, 0, 1, 0, 0, 0])"
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Ejercicio 2\n",
    "\n",
    "Prepara una función que va a recibir el nombre de la arquitectura que vamos a emplear para realizar transferencia de conocimiento. Vamos a probar con las siguientes arquitecturas: 'densenet', 'efficientnet_v2', 'inception_v3', 'mobilenet_v3', 'resnet_v2' y 'xception'.\n",
    "\n",
    "La función recibirá el nombre de la arquitectura como palabra clave. En función de esta, la función devolverá el modelo pre-entrenado (con los pesos de ImageNet y sin la última capa de clasificación) y la capa de pre-proceso correspondientes."
   ],
   "metadata": {
    "id": "MSx-eK7EFTJV"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def get_model(model):\n",
    "    match model:\n",
    "        case 'densenet':\n",
    "            model_constructor = tf.keras.applications.densenet.DenseNet121\n",
    "            preprocess = tf.keras.applications.densenet.preprocess_input\n",
    "        case 'efficientnet_v2':\n",
    "            model_constructor = tf.keras.applications.efficientnet_v2.EfficientNetV2S\n",
    "            preprocess = tf.keras.applications.efficientnet_v2.preprocess_input\n",
    "        case 'inception_v3':\n",
    "            model_constructor = tf.keras.applications.inception_v3.InceptionV3\n",
    "            preprocess = tf.keras.applications.inception_v3.preprocess_input\n",
    "        case 'mobilnet_v3':\n",
    "            model_constructor = tf.keras.applications.MobileNetV3Small\n",
    "            preprocess = tf.keras.applications.mobilenet_v3.preprocess_input\n",
    "        case 'resnet_v2':\n",
    "            model_constructor = tf.keras.applications.resnet_v2.ResNet101V2\n",
    "            preprocess = tf.keras.applications.resnet_v2.preprocess_input\n",
    "        case 'xception':\n",
    "            model_constructor = tf.keras.applications.xception.Xception\n",
    "            preprocess = tf.keras.applications.xception.Xception\n",
    "        case _:\n",
    "            print('Model does not exit')\n",
    "            exit(1)\n",
    "\n",
    "    model_base = model_constructor(include_top=False, weights='imagenet', input_shape=IMG_SIZE)\n",
    "\n",
    "    return model_base, preprocess"
   ],
   "metadata": {
    "id": "A5vd8aarCH2j",
    "ExecuteTime": {
     "end_time": "2024-03-06T18:21:37.720319Z",
     "start_time": "2024-03-06T18:21:37.714755Z"
    }
   },
   "execution_count": 136,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Ejercicio 3\n",
    "\n",
    "Construye una función que, reciba: el tamaño de la imagen de entrada, la capa de preproceso y el modelo pre-entrenado y, tras añadirle la capa densa correspondiente, devuelva el modelo que vamos a emplear para resolver el problema de clasificación.\n",
    "El modelo pre-entrenado tendrá todas las capas congeladas.\n",
    "\n"
   ],
   "metadata": {
    "id": "aHv2RQpDsy6k"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def add_first_layer(image_size, preprocess, model):\n",
    "    model.trainable = False\n",
    "\n",
    "    input_layer = layers.Input(image_size)\n",
    "    preprocess_layer = preprocess(input_layer)\n",
    "    base_layer = model(preprocess_layer, training=False)\n",
    "    flatter_layer = layers.Flatten()(base_layer)\n",
    "    output_layer = layers.Dense(1, activation=\"sigmoid\")(flatter_layer)\n",
    "\n",
    "    model_built = Model(inputs=[input_layer], outputs=[output_layer])\n",
    "\n",
    "    return model_built"
   ],
   "metadata": {
    "id": "gz-X8x9bSHrW",
    "ExecuteTime": {
     "end_time": "2024-03-06T18:21:37.725140Z",
     "start_time": "2024-03-06T18:21:37.721318Z"
    }
   },
   "execution_count": 137,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Ejercicio 4\n",
    "\n",
    "Construye una función que, dado un modelo, lo entrene durante 10 épocas y, tras este entrenamiento, descongele todo el modelo y congele solo la primera mitad de las capas y vuelva a entrenar durante 5 épocas más con una tasa de aprendizaje de 10^-5. Esta función deberá devolver la historia del entrenamiento."
   ],
   "metadata": {
    "id": "LGieBKW8SH-m"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def train_model(model, train_data, validation_data):\n",
    "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath='./Practice04.h5',\n",
    "        monitor='val_accuracy',\n",
    "        mode='max',\n",
    "        save_best_only=True,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    history_extraction = model.fit(train_data, epochs=10, validation_data=validation_data,\n",
    "                                   callbacks=[model_checkpoint_callback], verbose=True)\n",
    "\n",
    "    model.trainable = True\n",
    "    for layer in model.layers[:len(model.layers) // 2]:\n",
    "        layer.trainable = False\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5), loss='binary_crossentropy',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    history_fine_tuning = model.fit(train_data, epochs=5, validation_data=validation_data,\n",
    "                                    callbacks=[model_checkpoint_callback], verbose=True)\n",
    "\n",
    "    return history_fine_tuning"
   ],
   "metadata": {
    "id": "0eBtErMRTRHi",
    "ExecuteTime": {
     "end_time": "2024-03-06T18:21:37.731109Z",
     "start_time": "2024-03-06T18:21:37.726139Z"
    }
   },
   "execution_count": 138,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Ejercicio 5\n",
    "\n",
    "Ejecuta la función anterior para cada uno de los modelos definidos en el ejercicio 2. Para cada uno de ellos muestra la curvas de entrenamiento y validación y, tras todos los entrenamientos, muestre en una sola gráfica todas las curvas de validación.\n",
    "\n",
    "¿Con qué modelo te quedarías?"
   ],
   "metadata": {
    "id": "i8Zk1DdJTRh8"
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def plot_history(history):\n",
    "    figure = plt.figure(figsize=(15, 5))\n",
    "    ax = figure.add_subplot(1, 1, 1)\n",
    "\n",
    "    ax.plot(history.history['loss'], label='loss')\n",
    "    ax.plot(history.history['val_loss'], label='val_loss')\n",
    "    ax.set_xlabel('Epoch')\n",
    "    ax.set_ylabel('Loss')\n",
    "    ax.legend()\n",
    "\n",
    "    return figure"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-06T18:21:37.735802Z",
     "start_time": "2024-03-06T18:21:37.732108Z"
    }
   },
   "execution_count": 139
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "histories = {}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-03-06T18:21:37.740272Z",
     "start_time": "2024-03-06T18:21:37.736814Z"
    }
   },
   "execution_count": 140
  },
  {
   "cell_type": "code",
   "source": [
    "models = ['densenet', 'efficientnet_v2', 'inception_v3', 'mobilenet_v3', 'resnet_v2', 'xception']\n",
    "for model in models:\n",
    "    model_base, preprocess = get_model(model)\n",
    "    model_built = add_first_layer(IMG_SIZE, preprocess, model_base)\n",
    "    model_built.summary()\n",
    "    history = train_model(model_built, train_dataset, validation_dataset)\n",
    "    histories[model] = history"
   ],
   "metadata": {
    "id": "YXsB5zAuTvSb",
    "ExecuteTime": {
     "end_time": "2024-03-06T18:46:46.842216Z",
     "start_time": "2024-03-06T18:21:37.741277Z"
    }
   },
   "execution_count": 141,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_12\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_26 (InputLayer)       [(None, 256, 256, 3)]     0         \n",
      "                                                                 \n",
      " tf.math.truediv_24 (TFOpLa  (None, 256, 256, 3)       0         \n",
      " mbda)                                                           \n",
      "                                                                 \n",
      " tf.nn.bias_add_12 (TFOpLam  (None, 256, 256, 3)       0         \n",
      " bda)                                                            \n",
      "                                                                 \n",
      " tf.math.truediv_25 (TFOpLa  (None, 256, 256, 3)       0         \n",
      " mbda)                                                           \n",
      "                                                                 \n",
      " densenet121 (Functional)    (None, 8, 8, 1024)        7037504   \n",
      "                                                                 \n",
      " flatten_2 (Flatten)         (None, 65536)             0         \n",
      "                                                                 \n",
      " dense_12 (Dense)            (None, 1)                 65537     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 7103041 (27.10 MB)\n",
      "Trainable params: 65537 (256.00 KB)\n",
      "Non-trainable params: 7037504 (26.85 MB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.8539 - accuracy: 0.8240\n",
      "Epoch 1: val_accuracy improved from -inf to 0.97000, saving model to .\\Practice04.h5\n",
      "16/16 [==============================] - 30s 2s/step - loss: 0.8539 - accuracy: 0.8240 - val_loss: 0.1075 - val_accuracy: 0.9700\n",
      "Epoch 2/10\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.1520 - accuracy: 0.9700\n",
      "Epoch 2: val_accuracy did not improve from 0.97000\n",
      "16/16 [==============================] - 27s 2s/step - loss: 0.1520 - accuracy: 0.9700 - val_loss: 0.3242 - val_accuracy: 0.9450\n",
      "Epoch 3/10\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0225 - accuracy: 0.9940\n",
      "Epoch 3: val_accuracy did not improve from 0.97000\n",
      "16/16 [==============================] - 26s 2s/step - loss: 0.0225 - accuracy: 0.9940 - val_loss: 0.1303 - val_accuracy: 0.9700\n",
      "Epoch 4/10\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0034 - accuracy: 0.9980\n",
      "Epoch 4: val_accuracy improved from 0.97000 to 0.98000, saving model to .\\Practice04.h5\n",
      "16/16 [==============================] - 26s 2s/step - loss: 0.0034 - accuracy: 0.9980 - val_loss: 0.1243 - val_accuracy: 0.9800\n",
      "Epoch 5/10\n",
      "16/16 [==============================] - ETA: 0s - loss: 4.7825e-04 - accuracy: 1.0000\n",
      "Epoch 5: val_accuracy did not improve from 0.98000\n",
      "16/16 [==============================] - 26s 2s/step - loss: 4.7825e-04 - accuracy: 1.0000 - val_loss: 0.1273 - val_accuracy: 0.9800\n",
      "Epoch 6/10\n",
      "16/16 [==============================] - ETA: 0s - loss: 2.4202e-04 - accuracy: 1.0000\n",
      "Epoch 6: val_accuracy improved from 0.98000 to 0.98500, saving model to .\\Practice04.h5\n",
      "16/16 [==============================] - 27s 2s/step - loss: 2.4202e-04 - accuracy: 1.0000 - val_loss: 0.1294 - val_accuracy: 0.9850\n",
      "Epoch 7/10\n",
      "16/16 [==============================] - ETA: 0s - loss: 1.8826e-04 - accuracy: 1.0000\n",
      "Epoch 7: val_accuracy did not improve from 0.98500\n",
      "16/16 [==============================] - 29s 2s/step - loss: 1.8826e-04 - accuracy: 1.0000 - val_loss: 0.1291 - val_accuracy: 0.9850\n",
      "Epoch 8/10\n",
      "16/16 [==============================] - ETA: 0s - loss: 1.5578e-04 - accuracy: 1.0000\n",
      "Epoch 8: val_accuracy did not improve from 0.98500\n",
      "16/16 [==============================] - 30s 2s/step - loss: 1.5578e-04 - accuracy: 1.0000 - val_loss: 0.1287 - val_accuracy: 0.9850\n",
      "Epoch 9/10\n",
      "16/16 [==============================] - ETA: 0s - loss: 1.3979e-04 - accuracy: 1.0000\n",
      "Epoch 9: val_accuracy did not improve from 0.98500\n",
      "16/16 [==============================] - 30s 2s/step - loss: 1.3979e-04 - accuracy: 1.0000 - val_loss: 0.1285 - val_accuracy: 0.9850\n",
      "Epoch 10/10\n",
      "16/16 [==============================] - ETA: 0s - loss: 1.2708e-04 - accuracy: 1.0000\n",
      "Epoch 10: val_accuracy did not improve from 0.98500\n",
      "16/16 [==============================] - 30s 2s/step - loss: 1.2708e-04 - accuracy: 1.0000 - val_loss: 0.1285 - val_accuracy: 0.9850\n",
      "Epoch 1/5\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0015 - accuracy: 1.0000\n",
      "Epoch 1: val_accuracy did not improve from 0.98500\n",
      "16/16 [==============================] - 141s 8s/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.1296 - val_accuracy: 0.9750\n",
      "Epoch 2/5\n",
      "16/16 [==============================] - ETA: 0s - loss: 5.5408e-04 - accuracy: 1.0000\n",
      "Epoch 2: val_accuracy did not improve from 0.98500\n",
      "16/16 [==============================] - 119s 7s/step - loss: 5.5408e-04 - accuracy: 1.0000 - val_loss: 0.1143 - val_accuracy: 0.9800\n",
      "Epoch 3/5\n",
      "16/16 [==============================] - ETA: 0s - loss: 6.2025e-05 - accuracy: 1.0000\n",
      "Epoch 3: val_accuracy did not improve from 0.98500\n",
      "16/16 [==============================] - 123s 8s/step - loss: 6.2025e-05 - accuracy: 1.0000 - val_loss: 0.0985 - val_accuracy: 0.9850\n",
      "Epoch 4/5\n",
      "16/16 [==============================] - ETA: 0s - loss: 1.3719e-05 - accuracy: 1.0000\n",
      "Epoch 4: val_accuracy did not improve from 0.98500\n",
      "16/16 [==============================] - 108s 7s/step - loss: 1.3719e-05 - accuracy: 1.0000 - val_loss: 0.0948 - val_accuracy: 0.9850\n",
      "Epoch 5/5\n",
      "16/16 [==============================] - ETA: 0s - loss: 1.0387e-05 - accuracy: 1.0000\n",
      "Epoch 5: val_accuracy did not improve from 0.98500\n",
      "16/16 [==============================] - 121s 8s/step - loss: 1.0387e-05 - accuracy: 1.0000 - val_loss: 0.0988 - val_accuracy: 0.9850\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/efficientnet_v2/efficientnetv2-s_notop.h5\n",
      "82420632/82420632 [==============================] - 2s 0us/step\n",
      "Model: \"model_13\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_28 (InputLayer)       [(None, 256, 256, 3)]     0         \n",
      "                                                                 \n",
      " efficientnetv2-s (Function  (None, 8, 8, 1280)        20331360  \n",
      " al)                                                             \n",
      "                                                                 \n",
      " flatten_3 (Flatten)         (None, 81920)             0         \n",
      "                                                                 \n",
      " dense_13 (Dense)            (None, 1)                 81921     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 20413281 (77.87 MB)\n",
      "Trainable params: 81921 (320.00 KB)\n",
      "Non-trainable params: 20331360 (77.56 MB)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.3763 - accuracy: 0.9040\n",
      "Epoch 1: val_accuracy improved from -inf to 0.94500, saving model to .\\Practice04.h5\n",
      "16/16 [==============================] - 55s 3s/step - loss: 0.3763 - accuracy: 0.9040 - val_loss: 0.2219 - val_accuracy: 0.9450\n",
      "Epoch 2/10\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0392 - accuracy: 0.9860\n",
      "Epoch 2: val_accuracy improved from 0.94500 to 0.97500, saving model to .\\Practice04.h5\n",
      "16/16 [==============================] - 38s 2s/step - loss: 0.0392 - accuracy: 0.9860 - val_loss: 0.1744 - val_accuracy: 0.9750\n",
      "Epoch 3/10\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0222 - accuracy: 0.9940\n",
      "Epoch 3: val_accuracy did not improve from 0.97500\n",
      "16/16 [==============================] - 39s 2s/step - loss: 0.0222 - accuracy: 0.9940 - val_loss: 0.0854 - val_accuracy: 0.9750\n",
      "Epoch 4/10\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0131 - accuracy: 0.9960\n",
      "Epoch 4: val_accuracy improved from 0.97500 to 0.98500, saving model to .\\Practice04.h5\n",
      "16/16 [==============================] - 40s 3s/step - loss: 0.0131 - accuracy: 0.9960 - val_loss: 0.0672 - val_accuracy: 0.9850\n",
      "Epoch 5/10\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0082 - accuracy: 0.9940\n",
      "Epoch 5: val_accuracy did not improve from 0.98500\n",
      "16/16 [==============================] - 40s 3s/step - loss: 0.0082 - accuracy: 0.9940 - val_loss: 0.2300 - val_accuracy: 0.9600\n",
      "Epoch 6/10\n",
      "16/16 [==============================] - ETA: 0s - loss: 0.0079 - accuracy: 0.9980\n",
      "Epoch 6: val_accuracy did not improve from 0.98500\n",
      "16/16 [==============================] - 35s 2s/step - loss: 0.0079 - accuracy: 0.9980 - val_loss: 0.0769 - val_accuracy: 0.9800\n",
      "Epoch 7/10\n",
      "16/16 [==============================] - ETA: 0s - loss: 1.1501e-04 - accuracy: 1.0000\n",
      "Epoch 7: val_accuracy improved from 0.98500 to 0.99500, saving model to .\\Practice04.h5\n",
      "16/16 [==============================] - 32s 2s/step - loss: 1.1501e-04 - accuracy: 1.0000 - val_loss: 0.0501 - val_accuracy: 0.9950\n",
      "Epoch 8/10\n",
      "16/16 [==============================] - ETA: 0s - loss: 4.4841e-06 - accuracy: 1.0000\n",
      "Epoch 8: val_accuracy did not improve from 0.99500\n",
      "16/16 [==============================] - 31s 2s/step - loss: 4.4841e-06 - accuracy: 1.0000 - val_loss: 0.0550 - val_accuracy: 0.9950\n",
      "Epoch 9/10\n",
      "16/16 [==============================] - ETA: 0s - loss: 6.7833e-06 - accuracy: 1.0000\n",
      "Epoch 9: val_accuracy did not improve from 0.99500\n",
      "16/16 [==============================] - 34s 2s/step - loss: 6.7833e-06 - accuracy: 1.0000 - val_loss: 0.0563 - val_accuracy: 0.9950\n",
      "Epoch 10/10\n",
      "16/16 [==============================] - ETA: 0s - loss: 6.8449e-06 - accuracy: 1.0000\n",
      "Epoch 10: val_accuracy did not improve from 0.99500\n",
      "16/16 [==============================] - 41s 3s/step - loss: 6.8449e-06 - accuracy: 1.0000 - val_loss: 0.0562 - val_accuracy: 0.9950\n",
      "Epoch 1/5\n",
      "16/16 [==============================] - ETA: 0s - loss: 6.2912e-06 - accuracy: 1.0000\n",
      "Epoch 1: val_accuracy did not improve from 0.99500\n",
      "16/16 [==============================] - 56s 3s/step - loss: 6.2912e-06 - accuracy: 1.0000 - val_loss: 0.0527 - val_accuracy: 0.9950\n",
      "Epoch 2/5\n",
      "16/16 [==============================] - ETA: 0s - loss: 4.2084e-06 - accuracy: 1.0000\n",
      "Epoch 2: val_accuracy did not improve from 0.99500\n",
      "16/16 [==============================] - 41s 3s/step - loss: 4.2084e-06 - accuracy: 1.0000 - val_loss: 0.0512 - val_accuracy: 0.9950\n",
      "Epoch 3/5\n",
      "16/16 [==============================] - ETA: 0s - loss: 3.4602e-06 - accuracy: 1.0000\n",
      "Epoch 3: val_accuracy did not improve from 0.99500\n",
      "16/16 [==============================] - 41s 3s/step - loss: 3.4602e-06 - accuracy: 1.0000 - val_loss: 0.0533 - val_accuracy: 0.9950\n",
      "Epoch 4/5\n",
      "16/16 [==============================] - ETA: 0s - loss: 2.7871e-06 - accuracy: 1.0000\n",
      "Epoch 4: val_accuracy did not improve from 0.99500\n",
      "16/16 [==============================] - 41s 3s/step - loss: 2.7871e-06 - accuracy: 1.0000 - val_loss: 0.0532 - val_accuracy: 0.9950\n",
      "Epoch 5/5\n",
      "16/16 [==============================] - ETA: 0s - loss: 2.4227e-06 - accuracy: 1.0000\n",
      "Epoch 5: val_accuracy did not improve from 0.99500\n",
      "16/16 [==============================] - 41s 3s/step - loss: 2.4227e-06 - accuracy: 1.0000 - val_loss: 0.0533 - val_accuracy: 0.9950\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'keras.api._v2.keras.applications' has no attribute 'inceptiov_v3'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[141], line 3\u001B[0m\n\u001B[0;32m      1\u001B[0m models \u001B[38;5;241m=\u001B[39m [\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mdensenet\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mefficientnet_v2\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124minception_v3\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmobilenet_v3\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mresnet_v2\u001B[39m\u001B[38;5;124m'\u001B[39m, \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mxception\u001B[39m\u001B[38;5;124m'\u001B[39m]\n\u001B[0;32m      2\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m model \u001B[38;5;129;01min\u001B[39;00m models:\n\u001B[1;32m----> 3\u001B[0m     model_base, preprocess \u001B[38;5;241m=\u001B[39m \u001B[43mget_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mmodel\u001B[49m\u001B[43m)\u001B[49m\n\u001B[0;32m      4\u001B[0m     model_built \u001B[38;5;241m=\u001B[39m add_first_layer(IMG_SIZE, preprocess, model_base)\n\u001B[0;32m      5\u001B[0m     model_built\u001B[38;5;241m.\u001B[39msummary()\n",
      "Cell \u001B[1;32mIn[136], line 10\u001B[0m, in \u001B[0;36mget_model\u001B[1;34m(model)\u001B[0m\n\u001B[0;32m      8\u001B[0m     preprocess \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39mapplications\u001B[38;5;241m.\u001B[39mefficientnet_v2\u001B[38;5;241m.\u001B[39mpreprocess_input\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mcase\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124minception_v3\u001B[39m\u001B[38;5;124m'\u001B[39m:\n\u001B[1;32m---> 10\u001B[0m     model_constructor \u001B[38;5;241m=\u001B[39m \u001B[43mtf\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mkeras\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mapplications\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43minceptiov_v3\u001B[49m\u001B[38;5;241m.\u001B[39mInceptionV3\n\u001B[0;32m     11\u001B[0m     preprocess \u001B[38;5;241m=\u001B[39m tf\u001B[38;5;241m.\u001B[39mkeras\u001B[38;5;241m.\u001B[39mapplications\u001B[38;5;241m.\u001B[39minceptiov_v3\u001B[38;5;241m.\u001B[39mpreprocess_input\n\u001B[0;32m     12\u001B[0m \u001B[38;5;28;01mcase\u001B[39;00m \u001B[38;5;124m'\u001B[39m\u001B[38;5;124mmobilnet_v3\u001B[39m\u001B[38;5;124m'\u001B[39m:\n",
      "\u001B[1;31mAttributeError\u001B[0m: module 'keras.api._v2.keras.applications' has no attribute 'inceptiov_v3'"
     ]
    }
   ]
  }
 ]
}
