{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Transferencia de conocimiento\n",
    "\n",
    "En esta segunda práctica de transferencia de conocimiento vamos a practicar con distintas arquitecturas. Para ello vamos a emplear la base de datos de clasificación de imágenes en pnumonía vs normal: https://drive.google.com/file/d/1wgODntXVXLp5bS0IyP9HVpGOrykNpS3h/view\n",
    "\n",
    "Para la práctica debes de realizar los siguientes ejercicios:\n"
   ],
   "metadata": {
    "id": "waCyJW3rEcmc"
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from matplotlib.ticker import MultipleLocator\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing import image_dataset_from_directory\n",
    "import os\n",
    "\n",
    "IMG_SIZE = (256, 256, 3)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "RANDOM_STATE = 123"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-28T18:47:54.435345Z",
     "start_time": "2024-02-28T18:47:54.431324Z"
    }
   },
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Ejercicio 1\n",
    "\n",
    "Descarga la base de datos y prepara los datasets de entrenamiento y validación con un tamaño de imagen de 256x256 y un tamaño de lote de 32."
   ],
   "metadata": {
    "id": "SDVa8UIRFCAG"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def get_images():\n",
    "    path_dataset = Path('./chest_xray')\n",
    "    path_train = path_dataset / 'entrenamiento'\n",
    "    path_val = path_dataset / 'validacion'\n",
    "\n",
    "    train_dataset = image_dataset_from_directory(\n",
    "        path_train,\n",
    "        shuffle=True,\n",
    "        seed=RANDOM_STATE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        image_size=(IMG_SIZE[0], IMG_SIZE[1])\n",
    "    )\n",
    "\n",
    "    validation_dataset = image_dataset_from_directory(\n",
    "        path_val,\n",
    "        shuffle=True,\n",
    "        seed=RANDOM_STATE,\n",
    "        batch_size=BATCH_SIZE,\n",
    "        image_size=(IMG_SIZE[0], IMG_SIZE[1])\n",
    "    )\n",
    "\n",
    "    return train_dataset, validation_dataset\n",
    "\n",
    "\n",
    "train_dataset, validation_dataset = get_images()"
   ],
   "metadata": {
    "id": "WujAg2TuFBSh",
    "ExecuteTime": {
     "end_time": "2024-02-28T18:47:54.532912Z",
     "start_time": "2024-02-28T18:47:54.463585Z"
    }
   },
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 500 files belonging to 2 classes.\n",
      "Found 200 files belonging to 2 classes.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Ejercicio 2\n",
    "\n",
    "Prepara una función que va a recibir el nombre de la arquitectura que vamos a emplear para realizar transferencia de conocimiento. Vamos a probar con las siguientes arquitecturas: 'densenet', 'efficientnet_v2', 'inception_v3', 'mobilenet_v3', 'resnet_v2' y 'xception'.\n",
    "\n",
    "La función recibirá el nombre de la arquitectura como palabra clave. En función de esta, la función devolverá el modelo pre-entrenado (con los pesos de ImageNet y sin la última capa de clasificación) y la capa de pre-proceso correspondientes."
   ],
   "metadata": {
    "id": "MSx-eK7EFTJV"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def get_model(model):\n",
    "    match model:\n",
    "        case 'densenet':\n",
    "            model_constructor = tf.keras.applications.densenet.DenseNet121\n",
    "            preprocess = tf.keras.application.densenet.preprocess_input\n",
    "        case 'efficientnet_v2':\n",
    "            model_constructor = tf.keras.applications.efficientnet_v2.EfficientNetV2S\n",
    "            preprocess = tf.keras.application.efficientnet_v2.preprocess_input\n",
    "        case 'inception_v3':\n",
    "            model_constructor = tf.keras.applications.inceptiov_v3.InceptionV3\n",
    "            preprocess = tf.keras.application.inceptiov_v3.preprocess_input\n",
    "        case 'mobilnet_v3':\n",
    "            model_constructor = tf.keras.applications.MobileNetV3Small\n",
    "            preprocess = tf.keras.application.mobilenet_v3.preprocess_input\n",
    "        case 'resnet_v2':\n",
    "            model_constructor = tf.keras.applications.resnet_v2.ResNet101V2\n",
    "            preprocess = tf.keras.application.resnet_v2.preprocess_input\n",
    "        case 'xception':\n",
    "            model_constructor = tf.keras.applications.xception.Xception\n",
    "            preprocess = tf.keras.application.xception.Xception\n",
    "        case _:\n",
    "            print('Model does not exit')\n",
    "            exit(1)\n",
    "\n",
    "    model_base = model_constructor(include_top=False, weights='imagenet', input_shape=IMG_SIZE)\n",
    "\n",
    "    return model_base, preprocess"
   ],
   "metadata": {
    "id": "A5vd8aarCH2j",
    "ExecuteTime": {
     "end_time": "2024-02-28T18:47:54.537618Z",
     "start_time": "2024-02-28T18:47:54.534149Z"
    }
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Ejercicio 3\n",
    "\n",
    "Construye una función que, reciba: el tamaño de la imagen de entrada, la capa de preproceso y el modelo pre-entrenado y, tras añadirle la capa densa correspondiente, devuelva el modelo que vamos a emplear para resolver el problema de clasificación.\n",
    "El modelo pre-entrenado tendrá todas las capas congeladas.\n",
    "\n"
   ],
   "metadata": {
    "id": "aHv2RQpDsy6k"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def add_first_layer(image_size, preprocess, model):\n",
    "    model.trainable = False\n",
    "\n",
    "    input_layer = layers.Input(image_size)\n",
    "    preprocess_layer = preprocess(input_layer)\n",
    "    base_layer = model(preprocess_layer, training=False)\n",
    "    output_layer = layers.Dense(1, activation=\"sigmoid\")(base_layer)\n",
    "\n",
    "    model_built = Model(inputs=[input_layer], outputs=[output_layer])\n",
    "\n",
    "    return model_built"
   ],
   "metadata": {
    "id": "gz-X8x9bSHrW",
    "ExecuteTime": {
     "end_time": "2024-02-28T18:47:54.543967Z",
     "start_time": "2024-02-28T18:47:54.537704Z"
    }
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Ejercicio 4\n",
    "\n",
    "Construye una función que, dado un modelo, lo entrene durante 10 épocas y, tras este entrenamiento, descongele todo el modelo y congele solo la primera mitad de las capas y vuelva a entrenar durante 5 épocas más con una tasa de aprendizaje de 10^-5. Esta función deberá devolver la historia del entrenamiento."
   ],
   "metadata": {
    "id": "LGieBKW8SH-m"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def train_model(model, train_data, validation_data):\n",
    "    model_checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath='./Practice04.h5',\n",
    "        monitor='val_accuracy',\n",
    "        mode='max',\n",
    "        save_best_only=True,\n",
    "        verbose=True\n",
    "    )\n",
    "\n",
    "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    history_extraction = model.fit(train_data, epochs=10, validation_data=validation_dataset,\n",
    "                                   callbacks=[model_checkpoint_callback], verbose=True)\n",
    "\n",
    "    model.trainable = True\n",
    "    model.compile(optimizer='adam', learning_rate=1e-5, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "    history_fine_tuning = model.fit(train_data, epochs=5, validation_data=validation_dataset,\n",
    "                                    callbacks=[model_checkpoint_callback], verbose=True)\n",
    "\n",
    "    return history_fine_tuning\n"
   ],
   "metadata": {
    "id": "0eBtErMRTRHi",
    "ExecuteTime": {
     "end_time": "2024-02-28T18:47:54.546612Z",
     "start_time": "2024-02-28T18:47:54.545105Z"
    }
   },
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Ejercicio 5\n",
    "\n",
    "Ejecuta la función anterior para cada uno de los modelos definidos en el ejercicio 2. Para cada uno de ellos muestra la curvas de entrenamiento y validación y, tras todos los entrenamiento, muestre en una sola gráfica todas las curvas de validación.\n",
    "\n",
    "¿Con qué modelo te quedarías?"
   ],
   "metadata": {
    "id": "i8Zk1DdJTRh8"
   }
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "YXsB5zAuTvSb",
    "ExecuteTime": {
     "end_time": "2024-02-28T18:47:54.549093Z",
     "start_time": "2024-02-28T18:47:54.546612Z"
    }
   },
   "execution_count": 8,
   "outputs": []
  }
 ]
}
