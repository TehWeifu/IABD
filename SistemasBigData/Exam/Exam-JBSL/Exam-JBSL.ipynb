{
 "cells": [
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "from pyspark import SparkContext"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-09T17:16:02.808407300Z",
     "start_time": "2024-02-09T17:16:02.785968400Z"
    }
   },
   "id": "be33ce37935d6c26",
   "execution_count": 109
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "initial_id",
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "end_time": "2024-02-09T17:16:02.848264300Z",
     "start_time": "2024-02-09T17:16:02.788625Z"
    }
   },
   "outputs": [],
   "source": [
    "sc = SparkContext(\"local\", \"Number RDD App\")"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ejercicio 1: Carga el fichero con spark"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7959c232aa001f27"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "raw_text_file_rdd = sc.textFile('./texto.txt')\n",
    "text_file_rdd = raw_text_file_rdd.flatMap(lambda x: x.split())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-09T17:16:02.867747200Z",
     "start_time": "2024-02-09T17:16:02.848264300Z"
    }
   },
   "id": "1572bfb993446dad",
   "execution_count": 111
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ejercicio 2: Contar la frecuencia de palabras"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "5cff3c2aa04e3095"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "[('En', 2),\n ('el', 6),\n ('a�o', 1),\n ('2045,', 1),\n ('la', 9),\n ('humanidad', 2),\n ('se', 6),\n ('encuentra', 2),\n ('en', 6),\n ('un', 7),\n ('punto', 1),\n ('de', 13),\n ('inflexi�n.', 1),\n ('La', 2),\n ('tecnolog�a', 1),\n ('ha', 3),\n ('transformado', 1),\n ('radicalmente', 1),\n ('nuestras', 1),\n ('vidas.', 1),\n ('Desde', 1),\n ('los', 4),\n ('avances', 1),\n ('inteligencia', 1),\n ('artificial', 1),\n ('hasta', 1),\n ('colonizaci�n', 1),\n ('Marte,', 1),\n ('mundo', 1),\n ('experimentado', 1),\n ('cambios', 1),\n ('asombrosos.', 1),\n ('Sin', 1),\n ('embargo,', 1),\n ('junto', 1),\n ('con', 2),\n ('progreso,', 1),\n ('tambi�n', 1),\n ('han', 1),\n ('surgido', 1),\n ('desaf�os.', 1),\n ('brecha', 1),\n ('entre', 1),\n ('ricos', 1),\n ('y', 4),\n ('pobres', 1),\n ('ampliado,', 1),\n ('lucha', 1),\n ('por', 1),\n ('recursos', 1),\n ('naturales', 1),\n ('intensifica.', 1),\n ('medio', 1),\n ('este', 1),\n ('panorama,', 1),\n ('grupo', 1),\n ('cient�ficos', 1),\n ('descubre', 1),\n ('fen�meno', 1),\n ('inexplicable', 1),\n ('espacio', 1),\n ('exterior:', 1),\n ('una', 3),\n ('serie', 2),\n ('pulsos', 2),\n ('electromagn�ticos', 2),\n ('que', 5),\n ('parecen', 1),\n ('transmitir', 1),\n ('mensaje', 2),\n ('codificado.', 1),\n ('Mientras', 1),\n ('tanto,', 1),\n ('Tierra,', 1),\n ('joven', 1),\n ('genio', 1),\n ('inform�tica', 1),\n ('llamado', 1),\n ('Alex', 3),\n ('algoritmo', 1),\n ('misterioso', 1),\n ('desencadena', 1),\n ('eventos', 1),\n ('catastr�ficos.', 1),\n ('Con', 1),\n ('ayuda', 1),\n ('equipo', 1),\n ('ecl�ctico', 1),\n ('hackers,', 1),\n ('embarca', 1),\n ('b�squeda', 1),\n ('fren�tica', 1),\n ('para', 1),\n ('descifrar', 1),\n ('oculto', 1),\n ('antes', 2),\n ('sea', 1),\n ('demasiado', 1),\n ('tarde.', 1),\n ('Su', 1),\n ('viaje', 1),\n ('lo', 2),\n ('llevar�', 1),\n ('a', 2),\n ('trav�s', 1),\n ('paisajes', 1),\n ('virtuales', 1),\n ('mundos', 1),\n ('alternativos,', 1),\n ('desafiando', 1),\n ('todo', 1),\n ('cre�a', 1),\n ('saber', 1),\n ('sobre', 1),\n ('realidad', 1),\n ('universo.', 1),\n ('�Podr�', 1),\n ('descubrir', 1),\n ('verdad', 1),\n ('detr�s', 1),\n ('enigmas', 1),\n ('c�smicos', 1),\n ('enfrente', 1),\n ('su', 1),\n ('destino', 1),\n ('final?', 1)]"
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_count = text_file_rdd.map(lambda x: (x, 1)).reduceByKey(lambda x, y: x + y)\n",
    "word_count.collect()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-09T17:16:04.152017300Z",
     "start_time": "2024-02-09T17:16:02.863701Z"
    }
   },
   "id": "2885c5c8634439b9",
   "execution_count": 112
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ejercicio 3: Encontrar las palabras más comunes"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b1230ee2f08127bb"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "[('de', 13),\n ('la', 9),\n ('un', 7),\n ('el', 6),\n ('se', 6),\n ('en', 6),\n ('que', 5),\n ('los', 4),\n ('y', 4),\n ('ha', 3)]"
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_sorted = word_count.sortBy(lambda w_count: w_count[1], ascending=False)\n",
    "word_sorted.take(10)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-09T17:16:04.726685700Z",
     "start_time": "2024-02-09T17:16:04.149010100Z"
    }
   },
   "id": "dad9aefd10469ed1",
   "execution_count": 113
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ejercicio 4: Eliminar palabras comunes (stop words), donde stop Word es la siguiente lista"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "cf508f8af2c551a5"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "['humanidad',\n 'humanidad',\n 'inflexi�n.',\n 'tecnolog�a',\n 'radicalmente',\n 'nuestras',\n 'vidas.',\n 'artificial',\n 'mundo',\n 'experimentado',\n 'embargo,',\n 'junto',\n 'progreso,',\n 'brecha',\n 'entre',\n 'pobres',\n 'lucha',\n 'recursos',\n 'naturales',\n 'intensifica.',\n 'medio',\n 'panorama,',\n 'grupo',\n 'descubre',\n 'fen�meno',\n 'inexplicable',\n 'espacio',\n 'serie',\n 'serie',\n 'pulsos',\n 'pulsos',\n 'electromagn�ticos',\n 'electromagn�ticos',\n 'parecen',\n 'mientras',\n 'tierra,',\n 'genio',\n 'alex',\n 'alex',\n 'alex',\n 'algoritmo',\n 'misterioso',\n 'desencadena',\n 'catastr�ficos.',\n 'ayuda',\n 'ecl�ctico',\n 'fren�tica',\n 'descifrar',\n 'oculto',\n 'demasiado',\n 'llevar�',\n 'trav�s',\n 'mundos',\n 'alternativos,',\n 'cre�a',\n 'saber',\n 'sobre',\n 'universo.',\n 'descubrir',\n 'verdad',\n 'detr�s',\n 'enigmas',\n 'c�smicos',\n 'destino',\n 'final?',\n 'a�o',\n '2045,',\n 'encuentra',\n 'encuentra',\n 'punto',\n 'transformado',\n 'desde',\n 'avances',\n 'inteligencia',\n 'hasta',\n 'colonizaci�n',\n 'marte,',\n 'cambios',\n 'asombrosos.',\n 'sin',\n 'tambi�n',\n 'han',\n 'surgido',\n 'desaf�os.',\n 'ricos',\n 'ampliado,',\n 'este',\n 'cient�ficos',\n 'exterior:',\n 'transmitir',\n 'mensaje',\n 'mensaje',\n 'codificado.',\n 'tanto,',\n 'joven',\n 'inform�tica',\n 'llamado',\n 'eventos',\n 'equipo',\n 'hackers,',\n 'embarca',\n 'b�squeda',\n 'antes',\n 'antes',\n 'sea',\n 'tarde.',\n 'viaje',\n 'a',\n 'a',\n 'paisajes',\n 'virtuales',\n 'desafiando',\n 'todo',\n 'realidad',\n '�podr�',\n 'enfrente']"
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdd_stop_words = sc.parallelize(\n",
    "    [\"la\", \"de\", \"en\", \"y\", \"un\", \"se\", \"con\", \"el\", \"que\", \"su\", \"para\", \"es\", \"un\", \"por\", \"lo\", \"ha\",\n",
    "     \"del\", \"los\", \"una\", \"año\", \"las\"]\n",
    ")\n",
    "text_without_stop_words = text_file_rdd.map(lambda x: x.lower()).subtract(rdd_stop_words)\n",
    "text_without_stop_words.collect()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-09T17:16:07.927671100Z",
     "start_time": "2024-02-09T17:16:04.723678500Z"
    }
   },
   "id": "cb2cae7a553c976b",
   "execution_count": 114
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ejercicio 5: Número de palabras únicas y conteo de palabras"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "47b40d1dcdf9b524"
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "data": {
      "text/plain": "126"
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_unique_words = text_file_rdd.distinct().count()\n",
    "count_unique_words"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-09T17:16:09.080423Z",
     "start_time": "2024-02-09T17:16:07.921652800Z"
    }
   },
   "id": "eaa9ab3516eda1ce",
   "execution_count": 115
  },
  {
   "cell_type": "markdown",
   "source": [
    "Ejercicio 6: Encontrar la longitud promedio de las palabras por párrafo (separando por punto)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "d1f731a841916f"
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "def transform_word_into_len(line):\n",
    "    words = line.split()\n",
    "    return [len(x) for x in words]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-09T17:16:09.081426200Z",
     "start_time": "2024-02-09T17:16:09.074722500Z"
    }
   },
   "id": "4e76d366ea0e8f16",
   "execution_count": 116
  },
  {
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Words in paragraph 1 have a mean of 4.15 characters of length\n",
      "Words in paragraph 2 have a mean of 7.29 characters of length\n",
      "Words in paragraph 3 have a mean of 6.18 characters of length\n",
      "Words in paragraph 4 have a mean of 5.50 characters of length\n",
      "Words in paragraph 5 have a mean of 4.56 characters of length\n",
      "Words in paragraph 6 have a mean of 5.86 characters of length\n",
      "Words in paragraph 7 have a mean of 5.46 characters of length\n",
      "Words in paragraph 8 have a mean of 5.06 characters of length\n",
      "Words in paragraph 9 have a mean of 4.88 characters of length\n",
      "Words in paragraph 10 have a mean of 4.76 characters of length\n"
     ]
    }
   ],
   "source": [
    "text_file_lines = raw_text_file_rdd.flatMap(lambda x: x.split('.')).map(lambda x: x.strip())\n",
    "count_word_per_line = text_file_lines.map(transform_word_into_len).map(lambda x: (sum(x), len(x))).map(\n",
    "    lambda x: x[0] / x[1])\n",
    "paragraphs_average = count_word_per_line.collect()\n",
    "\n",
    "for idx, paragraph in enumerate(paragraphs_average):\n",
    "    print(f\"Words in paragraph {idx + 1} have a mean of {paragraph:.2f} characters of length\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-09T17:16:09.723321600Z",
     "start_time": "2024-02-09T17:16:09.080423Z"
    }
   },
   "id": "7db7234c01d5f477",
   "execution_count": 117
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "sc.stop()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-09T17:16:10.724551400Z",
     "start_time": "2024-02-09T17:16:09.721318400Z"
    }
   },
   "id": "979daee985a3e59f",
   "execution_count": 118
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
